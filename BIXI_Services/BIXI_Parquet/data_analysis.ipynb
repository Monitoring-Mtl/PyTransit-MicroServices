{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98757a92-0a60-4d4c-bec2-b9fedb243f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 #ASSURER VOUS QU'IL NE SOIT PAS DANS LE REQUIREMENTS.TXT\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import fastparquet\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pytz\n",
    "import os\n",
    "import shutil\n",
    "import io\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59b04e-ae66-4182-9fed-3c204115e7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d752fde5-7ef0-48b5-980a-bc9eea6b4b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>num_bikes_available</th>\n",
       "      <th>num_ebikes_available</th>\n",
       "      <th>num_bikes_disabled</th>\n",
       "      <th>num_docks_available</th>\n",
       "      <th>num_docks_disabled</th>\n",
       "      <th>is_installed</th>\n",
       "      <th>is_renting</th>\n",
       "      <th>is_returning</th>\n",
       "      <th>last_reported</th>\n",
       "      <th>eightd_has_available_keys</th>\n",
       "      <th>is_charging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706649965</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706651702</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706651957</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706651203</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706651251</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>850</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706590552</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>873</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706651977</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>886</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706652178</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>888</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706652183</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>86400</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    station_id  num_bikes_available  num_ebikes_available  num_bikes_disabled  \\\n",
       "0            1                    7                     0                   0   \n",
       "1            3                    3                     0                   0   \n",
       "2           11                    6                     0                   0   \n",
       "3           15                    3                     0                   0   \n",
       "4           19                   19                     0                   0   \n",
       "..         ...                  ...                   ...                 ...   \n",
       "158        850                    7                     0                   0   \n",
       "159        873                   18                     0                   0   \n",
       "160        886                    7                     0                   0   \n",
       "161        888                   11                     0                   0   \n",
       "162        894                    0                     0                   0   \n",
       "\n",
       "     num_docks_available  num_docks_disabled  is_installed  is_renting  \\\n",
       "0                     28                   0             1           1   \n",
       "1                     16                   0             1           1   \n",
       "2                     17                   0             1           1   \n",
       "3                     23                   0             1           1   \n",
       "4                      8                   0             1           1   \n",
       "..                   ...                 ...           ...         ...   \n",
       "158                    4                   0             1           1   \n",
       "159                   15                   0             1           1   \n",
       "160                   22                   0             1           1   \n",
       "161                   30                   0             1           1   \n",
       "162                    0                   0             0           1   \n",
       "\n",
       "     is_returning  last_reported  eightd_has_available_keys  is_charging  \n",
       "0               1     1706649965                      False        False  \n",
       "1               1     1706651702                      False        False  \n",
       "2               1     1706651957                      False        False  \n",
       "3               1     1706651203                      False        False  \n",
       "4               1     1706651251                      False        False  \n",
       "..            ...            ...                        ...          ...  \n",
       "158             1     1706590552                      False         True  \n",
       "159             1     1706651977                      False         True  \n",
       "160             1     1706652178                      False         True  \n",
       "161             1     1706652183                      False         True  \n",
       "162             1          86400                      False        False  \n",
       "\n",
       "[163 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file= 'gbfs_data_station_status_1706652274.parquet'\n",
    "df = pd.read_parquet(file, engine='pyarrow')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c19a45-599b-468c-8bee-81e378108108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c59f2-7bdc-427b-a935-e5704d38c9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da71ce-a002-4e8c-9d28-7e222f0e57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "\n",
    "    bucket_static_daily = event['daily_bucket_static']\n",
    "    bucket_vehicle_positions_daily_merge = event['bucket_vehicle_positions_daily_merge']\n",
    "    output_bucket = event['output_bucket']\n",
    "    timezone_str = event.get('timezone', 'America/Montreal')  # Default to 'America/Montreal' if not specified\n",
    "\n",
    "    eastern = pytz.timezone(timezone_str)\n",
    "\n",
    "    # Extract the date from the event, or use the current date in the specified timezone (Format YYYYMMDD)\n",
    "    date_str = event.get('date', datetime.now(eastern).strftime('%Y%m%d'))\n",
    "\n",
    "    # Parse the date string into a datetime object\n",
    "    date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "    date_obj = eastern.localize(date_obj)\n",
    "\n",
    "    next_day = date_obj + timedelta(days=1)\n",
    "\n",
    "    folder_name = date_obj.strftime('%Y/%m/%d')\n",
    "    folder_name_next_day = next_day.strftime('%Y/%m/%d')\n",
    "\n",
    "\n",
    "    file_name = date_obj.strftime('%Y-%m-%d')\n",
    "    file_name_next_day = next_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Define the name of the local files\n",
    "    local_static_file_name = f'./tmp/filtered_stop_times_{file_name}.parquet'\n",
    "    local_file_name = f'./tmp/Daily_merge_{file_name}.parquet'\n",
    "    local_file_name_next_day = f'./tmp/Daily_merge_{file_name_next_day}.parquet'\n",
    "\n",
    "    # Define the folder and file structure on S3\n",
    "    key_static = f'{folder_name}/filtered_stop_times/filtered_stop_times_{file_name}.parquet'\n",
    "    key = f'{folder_name}/Daily_GTFS_VehiclePosition_{file_name}.parquet'\n",
    "    key_next_day = f'{folder_name_next_day}/Daily_GTFS_VehiclePosition_{file_name_next_day}.parquet'\n",
    "\n",
    "    # Download files locally\n",
    "    local_path_static = download_file_to_tmp(bucket_static_daily, key_static, local_static_file_name)\n",
    "    local_day_path = download_file_to_tmp(bucket_vehicle_positions_daily_merge, key, local_file_name)\n",
    "    local_next_day_path = download_file_to_tmp(bucket_vehicle_positions_daily_merge, key_next_day, local_file_name_next_day)\n",
    "\n",
    "    df = pl.read_parquet(local_day_path)\n",
    "    df_next_day = pl.read_parquet(local_next_day_path)\n",
    "    df_stop_times = pl.read_parquet(local_path_static)\n",
    "\n",
    "    # Drop useless column\n",
    "    df_stop_times = df_stop_times.drop({'departure_time'})\n",
    "    # We create the new column 'arrival_time_unix' converting the time in UNIX.\n",
    "    df_stops_unix = adding_arrival_time_unix(df_stop_times, date_obj)\n",
    "\n",
    "    # Merge the two DFs (current_day + next_day) of VehiclePositions\n",
    "    dfs_daily_vehicle_positions_merge = pl.concat([df, df_next_day], rechunk=True)\n",
    "\n",
    "    # We rename a column and convert the type of others\n",
    "    dfs_daily_vehicle_positions_merge = rename_and_convert_columns(dfs_daily_vehicle_positions_merge)\n",
    "\n",
    "    #dfs_daily_vehicle_positions_merge.write_parquet('dfs_daily_vehicle_positions_merge.parquet') #Used to generate the map\n",
    "\n",
    "    # We proceed with the analysis of the DATA based on the daily stop_times file provided\n",
    "    df_processed = process_based_on_daily_static_files(dfs_daily_vehicle_positions_merge, df_stops_unix)\n",
    "\n",
    "    # We remove the duplicate rows\n",
    "    df_processed = (df_processed.unique(subset=['trip_id', 'vehicle_currentStopSequence'])\n",
    "                    .sort(['trip_id', 'vehicle_currentStopSequence', 'timefetch']))\n",
    "\n",
    "    df_processed = df_processed.with_columns(((pl.col('arrival_time_offset') + pl.col('departure_time_offset'))/2).alias('offset'))\n",
    "\n",
    "    # Select only the columns you want to keep from the right DataFrame\n",
    "    df_processed = df_processed.select([\n",
    "        'trip_id',\n",
    "        'vehicle_currentStopSequence',\n",
    "        'vehicle_currentStatus',\n",
    "        'id',\n",
    "        'vehicle_occupancyStatus',\n",
    "        'offset',\n",
    "        'vehicle_trip_routeId',\n",
    "        'arrival_time_offset',\n",
    "        'departure_time_offset'\n",
    "    ])\n",
    "\n",
    "    # Perform the join\n",
    "    df_final = df_stops_unix.join(\n",
    "        df_processed,\n",
    "        how='inner',\n",
    "        left_on=['trip_id', 'stop_sequence'],\n",
    "        right_on=['trip_id', 'vehicle_currentStopSequence']\n",
    "    )\n",
    "\n",
    "    # Drop a double column and rename some.\n",
    "    df_final = df_final.drop('vehicle_currentStatus')\n",
    "    df_final = df_final.rename({'id': 'vehicleID', 'vehicle_occupancyStatus': 'Current_Occupancy',\n",
    "                                'vehicle_trip_routeId': 'routeId'})\n",
    "    df_final = df_final.cast({'routeId': pl.Int64})\n",
    "\n",
    "    df_final.write_parquet(f'./tmp/data_stops_{file_name}.parquet')\n",
    "\n",
    "    # Upload the final file back to S3\n",
    "    output_key = f'{folder_name}/data_stops_{file_name}.parquet'  # Set your output file path here\n",
    "    upload_file_from_tmp(f'./tmp/data_stops_{file_name}.parquet', output_bucket, output_key)\n",
    "\n",
    "    clean_tmp_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7d6d0-0514-4eb9-8983-dce35bcf4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_to_tmp(bucket, key, local_file_name):\n",
    "    \"\"\"\n",
    "    Download a file from a S3 bucket and stores it locally\n",
    "    :param bucket: Bucket name in S3\n",
    "    :param key: the key of the file to retrieve\n",
    "    :param local_file_name: the path where to store it locally\n",
    "    :return: the local path if success, none if failed.\n",
    "    \"\"\"\n",
    "    local_path = local_file_name\n",
    "    try:\n",
    "        s3.download_file(Bucket=bucket, Key=key, Filename=local_path)\n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        print(f'Error downloading file from S3: {e}')\n",
    "        return False\n",
    "\n",
    "\n",
    "def upload_file_from_tmp(local_file_name, bucket, key):\n",
    "    \"\"\"\n",
    "    Uploads a file to an S3 bucket from the local file system.\n",
    "    :param local_file_name: the path of the local file\n",
    "    :param bucket: Bucket name in S3\n",
    "    :param key: the key under which to store the file\n",
    "    :return: True if file was uploaded successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        if os.path.exists(local_file_name):\n",
    "            s3.upload_file(Filename=local_file_name, Bucket=bucket, Key=key)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error: The file {local_file_name} does not exist.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'Error uploading file to S3: {e}')\n",
    "        return False\n",
    "\n",
    "\n",
    "def clean_tmp_folder():\n",
    "    tmp_dir = './tmp'\n",
    "    for item in os.listdir(tmp_dir):\n",
    "        item_path = os.path.join(tmp_dir, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}. Reason: {e}\")\n",
    "\n",
    "\n",
    "def adding_arrival_time_unix(df_temp, date_obj):\n",
    "    \"\"\"\n",
    "    Create a new column \"arrival_time_unix\", it uses the existing column \"arrival_time\" and using the day passed in\n",
    "    the event convert that time in a UNIX value for the timezone (also given in the event). In case where the value is\n",
    "    greater than 23:59:59, we increment the date by 1. ex: (event date: 2023-12-01) 25:54:00 -> 1:54:00 AM of 2023-12-02\n",
    "    and then convert that to a UNIX value\n",
    "    :param df_temp: Dataframe to apply the addition to\n",
    "    :return: Dataframe with the modification sorted by trip_id and arrival_time_unix\n",
    "    \"\"\"\n",
    "    df_temp = df_temp.with_columns(df_temp['arrival_time'].str.split_exact(':', 2).struct.\n",
    "                                         rename_fields([\"hours\", \"minutes\", \"seconds\"])\n",
    "                                         .alias('time_split')).unnest('time_split')\n",
    "\n",
    "    # Convert 'hours', 'minutes', and 'seconds' to integers\n",
    "    df_temp = df_temp.with_columns([\n",
    "        pl.col(\"hours\").cast(pl.Int32),\n",
    "        pl.col(\"minutes\").cast(pl.Int32),\n",
    "        pl.col(\"seconds\").cast(pl.Int32)\n",
    "    ])\n",
    "\n",
    "    # Adjust for hours >= 24 and calculate additional days\n",
    "    df_temp = df_temp.with_columns([\n",
    "        (pl.col(\"hours\") % 24).alias(\"adjusted_hours\"),\n",
    "        (pl.col(\"hours\") / 24).floor().cast(pl.Int32).alias(\"additional_days\")\n",
    "    ])\n",
    "\n",
    "    # Extract year, month, and day from the localized date_obj\n",
    "    year, month, day = date_obj.year, date_obj.month, date_obj.day\n",
    "\n",
    "    # Create DateTime objects and convert to UNIX timestamps\n",
    "    df_temp = df_temp.with_columns(\n",
    "        (pl.datetime(year, month, day, \"adjusted_hours\", \"minutes\", \"seconds\", time_zone='America/Montreal') +\n",
    "         pl.duration(days=\"additional_days\")).alias(\"datetime\")\n",
    "    )\n",
    "\n",
    "    # Convert to UNIX timestamp\n",
    "    df_temp = df_temp.with_columns(\n",
    "        pl.col(\"datetime\").dt.timestamp('ms').alias(\"arrival_time_unix\")\n",
    "    )\n",
    "\n",
    "    return df_temp.select(pl.col('trip_id'), pl.col('arrival_time'), pl.col('stop_id'), pl.col('stop_sequence')\n",
    "                          , (pl.col('arrival_time_unix') / 1000).cast(pl.Int64)).sort(['trip_id', 'stop_sequence'])\n",
    "\n",
    "\n",
    "def rename_and_convert_columns(df):\n",
    "    df_temp = df.rename({'vehicle_trip_tripId': 'trip_id'})\n",
    "    try:\n",
    "        df_temp = df_temp.cast({'id': pl.Int32,'timefetch': pl.Int64, 'vehicle_position_bearing': pl.Int32,\n",
    "                                'vehicle_position_latitude': pl.Float64, 'vehicle_position_longitude': pl.Float64,\n",
    "                               'vehicle_position_speed': pl.Float64, 'vehicle_timestamp': pl.Int64, 'trip_id': pl.Int64})\n",
    "    except Exception as e:\n",
    "        print(f'Error {e} converting columns type of {df}')\n",
    "        raise\n",
    "    return df_temp.sort(['trip_id', 'vehicle_currentStopSequence', 'timefetch'])\n",
    "\n",
    "\n",
    "def filter_daily_vehicle_position(df):\n",
    "    # Creating a column to identify where the vehicle_currentStopSequence changes\n",
    "    df_temp = df.with_columns(\n",
    "        pl.col(\"vehicle_currentStopSequence\").diff().ne(0).alias(\"stop_sequence_changed\")\n",
    "    )\n",
    "\n",
    "    # Filtering the DataFrame based on the specified conditions\n",
    "    filtered_merged_df = df_temp.filter(\n",
    "        (pl.col(\"stop_sequence_changed\") & (pl.col(\"vehicle_currentStatus\") == \"IN_TRANSIT_TO\")) |\n",
    "        (pl.col(\"vehicle_currentStatus\") == \"STOPPED_AT\")\n",
    "    )\n",
    "\n",
    "    return filtered_merged_df.drop('stop_sequence_changed')\n",
    "\n",
    "\n",
    "def calculate_offset_for_stopped_status(df):\n",
    "    \"\"\"\n",
    "    Calculate the offset for the stop_sequence where the vehicle_status is \"stopped_at\"\n",
    "    :param df: Dataframe to calculate the offset on\n",
    "    :return: Dataframe sorted by trip_id and vehicle_stop\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col('vehicle_currentStatus') == \"STOPPED_AT\")\n",
    "        .then(pl.col('vehicle_timestamp') - pl.col('arrival_time_unix'))\n",
    "        .otherwise(None)\n",
    "        .alias('offset')\n",
    "    ])\n",
    "\n",
    "    # return the DF sorted by 'trip_id' and 'vehicle_currentStopSequence'\n",
    "    return df.sort(['trip_id', 'vehicle_currentStopSequence'])\n",
    "\n",
    "\n",
    "def calculate_offset_for_in_transit_status(df):\n",
    "    # Use the `shift()` function to get the 'vehicle_timestamp' of the next 'vehicle_currentStopSequence'\n",
    "    # within each 'trip_id'. We shift by -1 to get the next value.\n",
    "    df = df.with_columns(\n",
    "        pl.col('vehicle_timestamp').shift(-1).over('trip_id').alias('next_vehicle_timestamp')\n",
    "    )\n",
    "\n",
    "    # Calculate the offset for 'IN_TRANSIT_TO' using the 'next_vehicle_timestamp'\n",
    "    # If the next row is from a different 'trip_id', we should not calculate the offset, so we also check for this\n",
    "    df = df.with_columns([\n",
    "        pl.when(\n",
    "            (pl.col('vehicle_currentStatus') == 'IN_TRANSIT_TO') &\n",
    "            (pl.col('trip_id') == pl.col('trip_id').shift(-1))\n",
    "        ).then(\n",
    "            pl.col('next_vehicle_timestamp') - pl.col('arrival_time_unix')\n",
    "        ).otherwise(\n",
    "            pl.col('offset')  # Keep the existing offset if the condition is not met\n",
    "        ).alias('offset')\n",
    "    ])\n",
    "\n",
    "    return df.sort(['trip_id', 'vehicle_currentStopSequence'])\n",
    "\n",
    "\n",
    "def calculate_offset_for_last_stop_sequence(df):\n",
    "    try:\n",
    "        # Create a column with the 'vehicle_timestamp' of the next row with the same 'vehicle_vehicle_id'\n",
    "        df = df.with_columns(\n",
    "            pl.when(pl.col('vehicle_vehicle_id') == pl.col('vehicle_vehicle_id').shift(-1))\n",
    "            .then(pl.col('vehicle_timestamp').shift(-1))\n",
    "            .otherwise(pl.lit(None))\n",
    "            .alias('next_vehicle_timestamp')\n",
    "        )\n",
    "\n",
    "        # Calculate the offset using the new 'next_vehicle_timestamp' column\n",
    "        # Make sure to compare the vehicle IDs to ensure they are the same before using the next timestamp\n",
    "        df = df.with_columns([\n",
    "            pl.when(\n",
    "                (pl.col('vehicle_currentStatus') != 'STOPPED_AT') &\n",
    "                (pl.col('vehicle_vehicle_id') == pl.col('vehicle_vehicle_id').shift(-1)) &\n",
    "                (pl.col('vehicle_currentStopSequence') == pl.col('vehicle_currentStopSequence').max().over('trip_id'))\n",
    "            ).then(\n",
    "                pl.col('next_vehicle_timestamp') - pl.col('arrival_time_unix')\n",
    "            ).otherwise(\n",
    "                pl.col('offset')\n",
    "            ).alias('offset')\n",
    "        ])\n",
    "        return df.sort(['trip_id', 'vehicle_currentStopSequence'])\n",
    "    except Exception as e:\n",
    "        print(f'Failed to calculate offset_for_the_last_stop_sequence {df}')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "\n",
    "def calculate_arrival_departure_offset(df):\n",
    "    \"\"\"\n",
    "    Calculate the arrival and departure time offset based on how many value there is for a stop_sequence\n",
    "    If we have more than one value of offsets for a stop_sequence, we have the information of arrival and departure\n",
    "    to that stop, we then take the lowest has arrival and highest as departure. If we have 1 value we assign that one\n",
    "    for both (arrival and departure offset)\n",
    "    :param df: Dataframe to calculate\n",
    "    :return: Dataframe with values added\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate the first and last offset for each group\n",
    "        first_offset = df.group_by(['trip_id', 'vehicle_currentStopSequence']).agg(\n",
    "            pl.first('offset').alias('arrival_time_offset')\n",
    "        ).with_columns(pl.col('vehicle_currentStopSequence').cast(pl.Int64))  # Cast if necessary\n",
    "\n",
    "        last_offset = df.group_by(['trip_id', 'vehicle_currentStopSequence']).agg(\n",
    "            pl.last('offset').alias('departure_time_offset')\n",
    "        ).with_columns(pl.col('vehicle_currentStopSequence').cast(pl.Int64))  # Cast if necessary\n",
    "\n",
    "        # Join the first and last offsets back to the original DataFrame\n",
    "        df = df.join(first_offset, on=['trip_id', 'vehicle_currentStopSequence'], how='left')\n",
    "        df = df.join(last_offset, on=['trip_id', 'vehicle_currentStopSequence'], how='left')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Failed to calculate arrival_departure_offset of {df}')\n",
    "        print(f'Error: {e}')\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_based_on_daily_static_files(dfs_daily_vehicle_positions_merge, df_stops_unix):\n",
    "    try:\n",
    "        # We filter to only keep the positions(rows) we need to process (remove duplicate)\n",
    "        df_filtered_vehicle_positions = filter_daily_vehicle_position(dfs_daily_vehicle_positions_merge)\n",
    "        #df_filtered_vehicle_positions.write_parquet('df_filtered_vehicle_positions.parquet')  # Used to generate the map\n",
    "\n",
    "        # We then reduce the number of rows to keep, only the one with value for a stop_sequence\n",
    "        df_merge = df_filtered_vehicle_positions.join(df_stops_unix, how='outer',\n",
    "                                                      left_on=['trip_id', 'vehicle_currentStopSequence'],\n",
    "                                                      right_on=['trip_id', 'stop_sequence'])\n",
    "\n",
    "        # We remove the rows of data that doesn't have an arrival_time_unix (see the documentation for why that would happen)\n",
    "        df_merge = df_merge.filter(pl.col('arrival_time_unix').is_not_null())\n",
    "        df_merge = df_merge.filter(pl.col('vehicle_timestamp').is_not_null())\n",
    "\n",
    "        # We create a column 'offset' and calculate a value to be able to the remove the vehiclePosition of the next day.\n",
    "        df_merge = df_merge.with_columns((pl.col('timefetch') - pl.col('arrival_time_unix')).alias('offset'))\n",
    "        # Filter out rows where the absolute value of the offset is greater than 7200 seconds(2 hours)(trip_id of the next_day)\n",
    "        df_time_difference = df_merge.filter(pl.col('offset').abs() <= 7200)\n",
    "        df_time_difference = df_time_difference.with_columns(pl.lit(None).alias('offset'))\n",
    "\n",
    "        # Calculate offset for \"STOPPED_AT\" VehicleStatus\n",
    "        df_time_difference = calculate_offset_for_stopped_status(df_time_difference)\n",
    "\n",
    "        # Calculate offset for \"IN_TRANSIT_TO\" VehicleStatus\n",
    "        df_time_difference = calculate_offset_for_in_transit_status(df_time_difference)\n",
    "\n",
    "        # Calculate offset for the last stop of a trip\n",
    "        df_time_difference = calculate_offset_for_last_stop_sequence(df_time_difference)\n",
    "\n",
    "        # Determine the arrival and departure offset for each stop if possible\n",
    "        df_time_difference = calculate_arrival_departure_offset(df_time_difference)\n",
    "\n",
    "        # Remove the stops with an offset of more than 1800 seconds (30 minutes)\n",
    "        df_time_difference = df_time_difference.filter(pl.col('offset').abs() <= 1800)\n",
    "\n",
    "        return df_time_difference.sort(['trip_id', 'vehicle_currentStopSequence', 'timefetch'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed to process {dfs_daily_vehicle_positions_merge} and {df_stops_unix} for daily static file')\n",
    "        print(f'Error: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a7f92-a14b-4aad-8e67-b14f236c3625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543689f-b6f7-4de0-8a61-726c5d4a9ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
