{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from asyncio import Queue\n",
    "from math import ceil\n",
    "from typing import Coroutine\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "from pymongo import InsertOne, UpdateOne\n",
    "import subprocess\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENT_TASKS = 4\n",
    "QUEUE_SIZE = 1\n",
    "BIXI_DATA_URL = \"https://bixi.com/en/open-data\"\n",
    "BIXI_CDN = \"https://s3.ca-central-1.amazonaws.com/cdn.bixi.com/\"\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "BIXI_DB_NAME = os.getenv(\"BIXI_DB_NAME\")\n",
    "BIXI_HISTORIC_URLS_COLLECTION = os.getenv(\"BIXI_HISTORIC_URLS_COLLECTION\")\n",
    "LOCATION_COLLECTION = os.getenv(\"LOCATION_COLLECTION\")\n",
    "TRIP_COLLECTION = os.getenv(\"TRIP_COLLECTION\")\n",
    "DEFAULT_ZIP_PATH = \"data/file.zip\"\n",
    "DEFAULT_EXTRACT_PATH = \"data/\"\n",
    "CHUNK_SIZE = 250000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip data model\n",
    "\n",
    "![csv](img/csv.png)\n",
    "\n",
    "In UC06, we want to be able to calculate the average trip duration on a given period of time.\n",
    "\n",
    "Trip duration average requires individual trip durations. These can be computed before loading the data in db.\n",
    "\n",
    "Station names are actually their location. And because stations might change name (location) over time, it's best if we use locations in the models.\n",
    "\n",
    "![model](img/model.png)\n",
    "\n",
    "For simplicity, we'll use the location names as id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation and loading\n",
    "\n",
    "There are several ways we can go about this.\n",
    "\n",
    "We could use the `mongoimport` command-line tool but it's not very flexible when it come to transforming the data. Other tools could be used along `mongoimport` to help on that aspect but the learning curve becomes steep. \n",
    "\n",
    "An alternative could be using `Panda` and `Pymongo` to efficiently transform and load the data into the db.\n",
    "\n",
    "### Testing `mongoimport`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongoimport_csv(csv_file, uri=MONGO_URI, db_name=BIXI_DB_NAME, collection=\"csv\"):\n",
    "    try:\n",
    "        command = [\n",
    "            'mongoimport',\n",
    "            '--uri', uri+\"?authSource=admin\",\n",
    "            '--db', db_name,\n",
    "            '--collection', collection,\n",
    "            '--type', 'csv',\n",
    "            '--file', csv_file,\n",
    "            '--headerline'\n",
    "        ]\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Error: {stderr.decode('utf-8')}\")\n",
    "        else:\n",
    "            print(f\"Output: {stdout.decode('utf-8')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Elapsed time 617.7822887897491 seconds\n"
     ]
    }
   ],
   "source": [
    "csv_file = DEFAULT_EXTRACT_PATH + \"DonneesOuvertes (1).csv\"\n",
    "t0 = time.time()\n",
    "mongoimport_csv(csv_file)\n",
    "t1 = time.time()\n",
    "elapsed_time = t1 - t0\n",
    "print(\"Elapsed time\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mongoimport](img/mongoimport.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pandas` and `Pymongo`\n",
    "\n",
    "The CSV files are quite large (`1.5GB+`, `10M+` rows) and we need to prioritize efficiency to keep our resource consumption to the minimum. \n",
    "\n",
    "Luckily we can process each row individually. But loading them all at once would require large memory capacity. A strategy is to read the data by chunks. We found by trial that `250000` rows per chunk doesn't overwhelm a modest device like a raspberry pi (4 cores and 4GB of memory) but under utilize the CPU capacity. It then comes naturally that we should process the chunks in parallel. \n",
    "\n",
    "So on one hand, we need to pace the amount of data we load in memory, and on the other, we need to load enough chunks to process them in parallel. Fortunately, ready-made solutions exist, including [asyncio Queues and Workers](https://docs.python.org/3/library/asyncio-queue.html).\n",
    "\n",
    "Our implementation looks like: \n",
    "\n",
    "- The `transform_load_csv` function iteratively reads `chunks` from the CSV file and schedules the processing tasks on a `queue` which has a limited size (1 by default). When the `queue` is full, the function must wait for a slot to open up before scheduling another task.\n",
    "\n",
    "![main](img/mainfunction.png)\n",
    "\n",
    "- The `workers` continuously poll the queue for available tasks and execute them. The `workers` work in parallel. We can adjust the number of `workers` and the `queue` size to control memory and processor usage to some extent.\n",
    "\n",
    "![work](img/work.png)\n",
    "\n",
    "- The location processing tasks extract location information from the received chunk according to the model and bulk write the generated documents in `MongoDB`. The location processing tasks don't bother removing duplicates because `MongoDB` prevents duplicated IDs anyway.\n",
    "\n",
    "![locations](img/locations.png)\n",
    "\n",
    "- The trip processing tasks remove information that are already extracted in location documents, keeping the station names for reference. The trip processing tasks then compute the duration of each trip in another column and bulk write the generated documents to `MongoDB`.\n",
    "\n",
    "![trips](img/trips.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def count_lines_async(filename):\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        \"wc\", \"-l\", filename, stdout=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    stdout, _ = await process.communicate()\n",
    "    lines = int(stdout.split()[0])\n",
    "    print(lines, \"lines\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "async def process_locations(chunk, station_location_collection):\n",
    "    stations = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                chunk[\n",
    "                    [\n",
    "                        \"STARTSTATIONNAME\",\n",
    "                        \"STARTSTATIONARRONDISSEMENT\",\n",
    "                        \"STARTSTATIONLATITUDE\",\n",
    "                        \"STARTSTATIONLONGITUDE\",\n",
    "                    ]\n",
    "                ].rename(\n",
    "                    columns={\n",
    "                        \"STARTSTATIONNAME\": \"name\",\n",
    "                        \"STARTSTATIONARRONDISSEMENT\": \"arrondissement\",\n",
    "                        \"STARTSTATIONLATITUDE\": \"latitude\",\n",
    "                        \"STARTSTATIONLONGITUDE\": \"longitude\",\n",
    "                    }\n",
    "                ),\n",
    "                chunk[\n",
    "                    [\n",
    "                        \"ENDSTATIONNAME\",\n",
    "                        \"ENDSTATIONARRONDISSEMENT\",\n",
    "                        \"ENDSTATIONLATITUDE\",\n",
    "                        \"ENDSTATIONLONGITUDE\",\n",
    "                    ]\n",
    "                ].rename(\n",
    "                    columns={\n",
    "                        \"ENDSTATIONNAME\": \"name\",\n",
    "                        \"ENDSTATIONARRONDISSEMENT\": \"arrondissement\",\n",
    "                        \"ENDSTATIONLATITUDE\": \"latitude\",\n",
    "                        \"ENDSTATIONLONGITUDE\": \"longitude\",\n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        .drop_duplicates(subset=[\"name\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    operations = [\n",
    "        UpdateOne(\n",
    "            {\"_id\": row[\"name\"]},\n",
    "            {\n",
    "                \"$setOnInsert\": {\n",
    "                    \"arrondissement\": row[\"arrondissement\"],\n",
    "                    \"latitude\": row[\"latitude\"],\n",
    "                    \"longitude\": row[\"longitude\"],\n",
    "                }\n",
    "            },\n",
    "            upsert=True,\n",
    "        )\n",
    "        for _, row in stations.iterrows()\n",
    "    ]\n",
    "    if operations:\n",
    "        await station_location_collection.bulk_write(operations, ordered=False)\n",
    "\n",
    "\n",
    "async def process_trips(chunk, trip_collection):\n",
    "    chunk[\"DURATION\"] = chunk[\"ENDTIMEMS\"] - chunk[\"STARTTIMEMS\"]\n",
    "    trip_docs = chunk[\n",
    "        [\"STARTSTATIONNAME\", \"ENDSTATIONNAME\", \"STARTTIMEMS\", \"ENDTIMEMS\", \"DURATION\"]\n",
    "    ].to_dict(\"records\")\n",
    "    if trip_docs:\n",
    "        operations = [InsertOne(doc) for doc in trip_docs]\n",
    "        await trip_collection.bulk_write(operations, ordered=False)\n",
    "\n",
    "\n",
    "async def do(task: Coroutine, chunk, index, collection):\n",
    "    print(\"🚀 started:\", task.__name__, index)\n",
    "    result = await task(chunk, collection)\n",
    "    print(task.__name__, index, \"✅\")\n",
    "    return result\n",
    "\n",
    "\n",
    "async def queue_task(task: Coroutine, chunk, index, collection, queue: Queue):\n",
    "    await queue.put(\n",
    "        (\n",
    "            do,\n",
    "            (\n",
    "                task,\n",
    "                chunk,\n",
    "                index,\n",
    "                collection,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(\"➡️ queued:\", task.__name__, index)\n",
    "\n",
    "\n",
    "async def worker(queue: Queue):\n",
    "    while True:\n",
    "        task_func, args = await queue.get()\n",
    "        await task_func(*args)\n",
    "        queue.task_done()\n",
    "\n",
    "\n",
    "async def transform_load_csv(\n",
    "    filename,\n",
    "    path=DEFAULT_EXTRACT_PATH,\n",
    "    mongo_uri=MONGO_URI,\n",
    "    db_name=BIXI_DB_NAME,\n",
    "    locations_collection=LOCATION_COLLECTION,\n",
    "    trips_collection=TRIP_COLLECTION,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    concurrent_tasks=CONCURRENT_TASKS,\n",
    "    queue_size=QUEUE_SIZE,\n",
    "):\n",
    "    print(\n",
    "        \"transform_load_csv\",\n",
    "        filename,\n",
    "        \"chunk_size\",\n",
    "        chunk_size,\n",
    "        \"concurrency\",\n",
    "        concurrent_tasks,\n",
    "        \"queue_size\",\n",
    "        queue_size,\n",
    "    )\n",
    "    client = AsyncIOMotorClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    location_collection = db[locations_collection]\n",
    "    trip_collection = db[trips_collection]\n",
    "    csv_file_path = os.path.join(path, filename)\n",
    "\n",
    "    queue = Queue(maxsize=queue_size)\n",
    "    workers = [asyncio.create_task(worker(queue)) for _ in range(concurrent_tasks)]\n",
    "\n",
    "    nb_lines = await count_lines_async(csv_file_path)\n",
    "    total_chunks = ceil(nb_lines / chunk_size)\n",
    "    print(total_chunks, \"chunks\")\n",
    "\n",
    "    for index, chunk in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size)):\n",
    "        await queue_task(process_locations, chunk, index, location_collection, queue)\n",
    "        await queue_task(process_trips, chunk, index, trip_collection, queue)\n",
    "\n",
    "    await queue.join()\n",
    "    for w in workers:\n",
    "        w.cancel()\n",
    "    await asyncio.gather(*workers, return_exceptions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_load_csv DonneesOuvertes (1).csv chunk_size 250000 concurrency 4 queue_size 1\n",
      "11790841 lines\n",
      "48 chunks\n",
      "➡️ queued: process_locations 0\n",
      "🚀 started: process_locations 0\n",
      "➡️ queued: process_trips 0\n",
      "🚀 started: process_trips 0\n",
      "➡️ queued: process_locations 1\n",
      "process_locations 0 ✅\n",
      "🚀 started: process_locations 1\n",
      "➡️ queued: process_trips 1\n",
      "🚀 started: process_trips 1\n",
      "➡️ queued: process_locations 2\n",
      "process_locations 1 ✅\n",
      "🚀 started: process_locations 2\n",
      "➡️ queued: process_trips 2\n",
      "🚀 started: process_trips 2\n",
      "➡️ queued: process_locations 3\n",
      "process_locations 2 ✅\n",
      "🚀 started: process_locations 3\n",
      "➡️ queued: process_trips 3\n",
      "process_locations 3 ✅\n",
      "🚀 started: process_trips 3\n",
      "➡️ queued: process_locations 4\n",
      "process_trips 0 ✅\n",
      "🚀 started: process_locations 4\n",
      "➡️ queued: process_trips 4\n",
      "process_locations 4 ✅\n",
      "🚀 started: process_trips 4\n",
      "➡️ queued: process_locations 5\n",
      "process_trips 1 ✅\n",
      "🚀 started: process_locations 5\n",
      "➡️ queued: process_trips 5\n",
      "process_locations 5 ✅\n",
      "🚀 started: process_trips 5\n",
      "➡️ queued: process_locations 6\n",
      "process_trips 2 ✅\n",
      "🚀 started: process_locations 6\n",
      "➡️ queued: process_trips 6\n",
      "process_locations 6 ✅\n",
      "🚀 started: process_trips 6\n",
      "➡️ queued: process_locations 7\n",
      "process_trips 3 ✅\n",
      "🚀 started: process_locations 7\n",
      "➡️ queued: process_trips 7\n",
      "process_locations 7 ✅\n",
      "🚀 started: process_trips 7\n",
      "➡️ queued: process_locations 8\n",
      "process_trips 4 ✅\n",
      "🚀 started: process_locations 8\n",
      "➡️ queued: process_trips 8\n",
      "process_locations 8 ✅\n",
      "🚀 started: process_trips 8\n",
      "➡️ queued: process_locations 9\n",
      "process_trips 5 ✅\n",
      "🚀 started: process_locations 9\n",
      "➡️ queued: process_trips 9\n",
      "process_locations 9 ✅\n",
      "🚀 started: process_trips 9\n",
      "➡️ queued: process_locations 10\n",
      "process_trips 7 ✅\n",
      "🚀 started: process_locations 10\n",
      "➡️ queued: process_trips 10\n",
      "process_trips 6 ✅\n",
      "🚀 started: process_trips 10\n",
      "➡️ queued: process_locations 11\n",
      "process_locations 10 ✅\n",
      "🚀 started: process_locations 11\n",
      "➡️ queued: process_trips 11\n",
      "process_locations 11 ✅\n",
      "🚀 started: process_trips 11\n",
      "➡️ queued: process_locations 12\n",
      "process_trips 8 ✅\n",
      "🚀 started: process_locations 12\n",
      "➡️ queued: process_trips 12\n",
      "process_trips 9 ✅\n",
      "🚀 started: process_trips 12\n",
      "process_locations 12 ✅\n",
      "➡️ queued: process_locations 13\n",
      "🚀 started: process_locations 13\n",
      "➡️ queued: process_trips 13\n",
      "process_locations 13 ✅\n",
      "🚀 started: process_trips 13\n",
      "➡️ queued: process_locations 14\n",
      "process_trips 10 ✅\n",
      "🚀 started: process_locations 14\n",
      "➡️ queued: process_trips 14\n",
      "process_locations 14 ✅\n",
      "🚀 started: process_trips 14\n",
      "➡️ queued: process_locations 15\n",
      "process_trips 11 ✅\n",
      "🚀 started: process_locations 15\n",
      "➡️ queued: process_trips 15\n",
      "process_locations 15 ✅\n",
      "🚀 started: process_trips 15\n",
      "➡️ queued: process_locations 16\n",
      "process_trips 12 ✅\n",
      "🚀 started: process_locations 16\n",
      "➡️ queued: process_trips 16\n",
      "process_locations 16 ✅\n",
      "🚀 started: process_trips 16\n",
      "➡️ queued: process_locations 17\n",
      "process_trips 13 ✅\n",
      "🚀 started: process_locations 17\n",
      "➡️ queued: process_trips 17\n",
      "process_locations 17 ✅\n",
      "🚀 started: process_trips 17\n",
      "➡️ queued: process_locations 18\n",
      "process_trips 15 ✅\n",
      "🚀 started: process_locations 18\n",
      "➡️ queued: process_trips 18\n",
      "process_locations 18 ✅\n",
      "🚀 started: process_trips 18\n",
      "➡️ queued: process_locations 19\n",
      "process_trips 14 ✅\n",
      "🚀 started: process_locations 19\n",
      "➡️ queued: process_trips 19\n",
      "process_locations 19 ✅\n",
      "🚀 started: process_trips 19\n",
      "➡️ queued: process_locations 20\n",
      "process_trips 16 ✅\n",
      "🚀 started: process_locations 20\n",
      "➡️ queued: process_trips 20\n",
      "process_locations 20 ✅\n",
      "🚀 started: process_trips 20\n",
      "➡️ queued: process_locations 21\n",
      "process_trips 17 ✅\n",
      "🚀 started: process_locations 21\n",
      "➡️ queued: process_trips 21\n",
      "process_locations 21 ✅\n",
      "🚀 started: process_trips 21\n",
      "➡️ queued: process_locations 22\n",
      "process_trips 18 ✅\n",
      "🚀 started: process_locations 22\n",
      "➡️ queued: process_trips 22\n",
      "process_locations 22 ✅\n",
      "🚀 started: process_trips 22\n",
      "➡️ queued: process_locations 23\n",
      "process_trips 19 ✅\n",
      "🚀 started: process_locations 23\n",
      "➡️ queued: process_trips 23\n",
      "process_locations 23 ✅\n",
      "🚀 started: process_trips 23\n",
      "➡️ queued: process_locations 24\n",
      "process_trips 20 ✅\n",
      "🚀 started: process_locations 24\n",
      "➡️ queued: process_trips 24\n",
      "process_locations 24 ✅\n",
      "🚀 started: process_trips 24\n",
      "➡️ queued: process_locations 25\n",
      "process_trips 21 ✅\n",
      "🚀 started: process_locations 25\n",
      "➡️ queued: process_trips 25\n",
      "process_locations 25 ✅\n",
      "🚀 started: process_trips 25\n",
      "➡️ queued: process_locations 26\n",
      "process_trips 22 ✅\n",
      "🚀 started: process_locations 26\n",
      "➡️ queued: process_trips 26\n",
      "process_locations 26 ✅\n",
      "🚀 started: process_trips 26\n",
      "➡️ queued: process_locations 27\n",
      "process_trips 23 ✅\n",
      "🚀 started: process_locations 27\n",
      "➡️ queued: process_trips 27\n",
      "process_locations 27 ✅\n",
      "🚀 started: process_trips 27\n",
      "➡️ queued: process_locations 28\n",
      "process_trips 24 ✅\n",
      "🚀 started: process_locations 28\n",
      "➡️ queued: process_trips 28\n",
      "process_locations 28 ✅\n",
      "🚀 started: process_trips 28\n",
      "➡️ queued: process_locations 29\n",
      "process_trips 25 ✅\n",
      "🚀 started: process_locations 29\n",
      "➡️ queued: process_trips 29\n",
      "process_locations 29 ✅\n",
      "🚀 started: process_trips 29\n",
      "➡️ queued: process_locations 30\n",
      "process_trips 26 ✅\n",
      "🚀 started: process_locations 30\n",
      "➡️ queued: process_trips 30\n",
      "process_locations 30 ✅\n",
      "🚀 started: process_trips 30\n",
      "➡️ queued: process_locations 31\n",
      "process_trips 28 ✅\n",
      "🚀 started: process_locations 31\n",
      "➡️ queued: process_trips 31\n",
      "process_locations 31 ✅\n",
      "🚀 started: process_trips 31\n",
      "➡️ queued: process_locations 32\n",
      "process_trips 27 ✅\n",
      "🚀 started: process_locations 32\n",
      "➡️ queued: process_trips 32\n",
      "process_locations 32 ✅\n",
      "🚀 started: process_trips 32\n",
      "➡️ queued: process_locations 33\n",
      "process_trips 29 ✅\n",
      "🚀 started: process_locations 33\n",
      "➡️ queued: process_trips 33\n",
      "process_locations 33 ✅\n",
      "🚀 started: process_trips 33\n",
      "➡️ queued: process_locations 34\n",
      "process_trips 30 ✅\n",
      "🚀 started: process_locations 34\n",
      "➡️ queued: process_trips 34\n",
      "process_locations 34 ✅\n",
      "🚀 started: process_trips 34\n",
      "➡️ queued: process_locations 35\n",
      "process_trips 31 ✅\n",
      "🚀 started: process_locations 35\n",
      "➡️ queued: process_trips 35\n",
      "process_locations 35 ✅\n",
      "🚀 started: process_trips 35\n",
      "➡️ queued: process_locations 36\n",
      "process_trips 32 ✅\n",
      "🚀 started: process_locations 36\n",
      "➡️ queued: process_trips 36\n",
      "process_locations 36 ✅\n",
      "🚀 started: process_trips 36\n",
      "➡️ queued: process_locations 37\n",
      "process_trips 34 ✅\n",
      "🚀 started: process_locations 37\n",
      "➡️ queued: process_trips 37\n",
      "process_locations 37 ✅\n",
      "🚀 started: process_trips 37\n",
      "➡️ queued: process_locations 38\n",
      "process_trips 33 ✅\n",
      "🚀 started: process_locations 38\n",
      "➡️ queued: process_trips 38\n",
      "process_locations 38 ✅\n",
      "🚀 started: process_trips 38\n",
      "➡️ queued: process_locations 39\n",
      "process_trips 35 ✅\n",
      "🚀 started: process_locations 39\n",
      "➡️ queued: process_trips 39\n",
      "process_locations 39 ✅\n",
      "🚀 started: process_trips 39\n",
      "➡️ queued: process_locations 40\n",
      "process_trips 36 ✅\n",
      "🚀 started: process_locations 40\n",
      "➡️ queued: process_trips 40\n",
      "process_locations 40 ✅\n",
      "🚀 started: process_trips 40\n",
      "➡️ queued: process_locations 41\n",
      "process_trips 37 ✅\n",
      "🚀 started: process_locations 41\n",
      "➡️ queued: process_trips 41\n",
      "process_locations 41 ✅\n",
      "🚀 started: process_trips 41\n",
      "➡️ queued: process_locations 42\n",
      "process_trips 38 ✅\n",
      "🚀 started: process_locations 42\n",
      "➡️ queued: process_trips 42\n",
      "process_locations 42 ✅\n",
      "🚀 started: process_trips 42\n",
      "➡️ queued: process_locations 43\n",
      "process_trips 39 ✅\n",
      "🚀 started: process_locations 43\n",
      "➡️ queued: process_trips 43\n",
      "process_locations 43 ✅\n",
      "🚀 started: process_trips 43\n",
      "➡️ queued: process_locations 44\n",
      "process_trips 40 ✅\n",
      "🚀 started: process_locations 44\n",
      "➡️ queued: process_trips 44\n",
      "process_locations 44 ✅\n",
      "🚀 started: process_trips 44\n",
      "➡️ queued: process_locations 45\n",
      "process_trips 41 ✅\n",
      "🚀 started: process_locations 45\n",
      "➡️ queued: process_trips 45\n",
      "process_locations 45 ✅\n",
      "🚀 started: process_trips 45\n",
      "➡️ queued: process_locations 46\n",
      "process_trips 43 ✅\n",
      "🚀 started: process_locations 46\n",
      "➡️ queued: process_trips 46\n",
      "process_locations 46 ✅\n",
      "🚀 started: process_trips 46\n",
      "➡️ queued: process_locations 47\n",
      "process_trips 42 ✅\n",
      "🚀 started: process_locations 47\n",
      "➡️ queued: process_trips 47\n",
      "process_locations 47 ✅\n",
      "🚀 started: process_trips 47\n",
      "process_trips 47 ✅\n",
      "process_trips 44 ✅\n",
      "process_trips 45 ✅\n",
      "process_trips 46 ✅\n",
      "Elapsed time 182.01650309562683 seconds\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"DonneesOuvertes (1).csv\"\n",
    "t0 = time.time()\n",
    "await transform_load_csv(csv_file)\n",
    "t1 = time.time()\n",
    "elapsed_time = t1 - t0\n",
    "print(\"Elapsed time\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pandapymongo](img/pandaspymongo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, our `transform_load_csv` python function not only is faster but manages storage space more efficiently than `mongoimport`:\n",
    "\n",
    "| Metric               | `mongoimport`      | `transform_load_csv` |\n",
    "|----------------------|--------------------|----------------------|\n",
    "| Transform and Load   | 10 mins            | 3 mins               |\n",
    "| Database Size After  | 1.52GB             | ~0.8GB               |\n",
    "\n",
    "⚠️ These tests were conducted with a high performance machine and a local instance of Mongodb. They are not representative of our production environment where we need to consider round trip delays to `Atlas` and limit our resource consumption. \n",
    "\n",
    "### Test on pi\n",
    "\n",
    "```bash\n",
    "(myenv) ➜  pytransit-bixi-extract du -sh . && /usr/bin/time -v python transform_load.py \"DonneesOuvertes.csv\" && du -sh .\n",
    "2.2G    .\n",
    "transform_load_csv DonneesOuvertes.csv chunk_size 250000 concurrency 10 queue_size 1\n",
    "11790841 lines\n",
    "48 chunks\n",
    "        Command being timed: \"python transform_load.py DonneesOuvertes.csv\"\n",
    "        User time (seconds): 507.42\n",
    "        System time (seconds): 47.30\n",
    "        Percent of CPU this job got: 107%\n",
    "        Elapsed (wall clock) time (h:mm:ss or m:ss): 8:37.03\n",
    "        Average shared text size (kbytes): 0\n",
    "        Average unshared data size (kbytes): 0\n",
    "        Average stack size (kbytes): 0\n",
    "        Average total size (kbytes): 0\n",
    "        Maximum resident set size (kbytes): 941552\n",
    "        Average resident set size (kbytes): 0\n",
    "        Major (requiring I/O) page faults: 0\n",
    "        Minor (reclaiming a frame) page faults: 1134596\n",
    "        Voluntary context switches: 596784\n",
    "        Involuntary context switches: 42199\n",
    "        Swaps: 0\n",
    "        File system inputs: 2596768\n",
    "        File system outputs: 0\n",
    "        Socket messages sent: 0\n",
    "        Socket messages received: 0\n",
    "        Signals delivered: 0\n",
    "        Page size (bytes): 4096\n",
    "        Exit status: 0\n",
    "2.2G    .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the test collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_test_collections():\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[BIXI_DB_NAME]\n",
    "    db[TRIP_COLLECTION].drop()\n",
    "    db[LOCATION_COLLECTION].drop()\n",
    "    db[\"csv\"].drop()\n",
    "\n",
    "drop_test_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encountered issues\n",
    "\n",
    "- We use a free Atlas tier database for tests. It has a size limit of `~500MB`. The free tier also uses shared public resources like network, CPU.. so writing to the database can be slow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytransit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
